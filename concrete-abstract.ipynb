{
 "cells": [
  {
   "cell_type": "raw",
   "id": "252fa8cc",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "To create a set of abstract word embeddings from a set of concrete word embeddings.\n",
    "\n",
    "Strategy:\n",
    "We have three datasets we'll be using:\n",
    "[wac2vec]\n",
    "[WordNet]\n",
    "[Concreteness Ratings]\n",
    "\n",
    "\n",
    "Let's create a class to store these and hold functions for manipulating them:\n",
    "[ConcreteAbstract]\n",
    " - isa: class\n",
    " - anchor\n",
    "\n",
    "[AbstractionTree]\n",
    " - anchor\n",
    " - a \"tree\"\n",
    "  - each leaf has a concrete word, embedding, abstraction level\n",
    "  - every parent is an abstract word one (or more?) abstraction level(s) above its children\n",
    "   - contains a classifier that can classify its children\n",
    " - pandas dataframe\n",
    "  - I think pandas will have some useful functions\n",
    "   \n",
    "\n",
    "What are some possible functions we may need?\n",
    "init_abstraction_tree()\n",
    " - takes the three datasets and starts a new tree with the root\n",
    "\n",
    "build_abstraction_tree()\n",
    " - takes the three datasets and builds the [AbstractionTree]\n",
    "  - starts with the leaves and builds towards the root(s)\n",
    " - we can try different kinds of trees using variations of this function\n",
    "\n",
    "\n",
    "build_classifiers()\n",
    " - takes an [AbstractionTree] and builds classifiers for the abstract words\n",
    "\n",
    "\n",
    "\n",
    "Wrapper classes for data?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86653052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/crow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "#import sklearn\n",
    "#from sklearn import metrics\n",
    "#from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "#from wac import WAC\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "#from sklearn import neural_network\n",
    "#import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict as dd\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "from operator import itemgetter\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4699bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#concreteness_threshold = 2\n",
    "concreteness_threshold = 7\n",
    "\n",
    "# Number of positive examples we want for each classifier\n",
    "#pos_count = 3\n",
    "#neg_count = 9\n",
    "pos_count = 10\n",
    "neg_count = 20\n",
    "#pos_count = 15\n",
    "#neg_count = 40\n",
    "#pos_count = 30\n",
    "#neg_count = 90\n",
    "\n",
    "\n",
    "test_pct = 0.33 #0.33\n",
    "\n",
    "embedding_file = 'ddata/clip.bertvocab.embeddings.513.txt'\n",
    "embedding_pickle = 'ddata/clip.bertvocab.embeddings.513.pkl'\n",
    "concreteness_file = 'ddata/AC_ratings_google3m_koeper_SiW.csv'\n",
    "concreteness_pickle = 'ddata/AC_ratings_google3m_koeper_SiW.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3369d5",
   "metadata": {},
   "source": [
    "## Initiate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f61ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wac2vec\n",
    "\n",
    "if os.path.isfile(embedding_pickle):\n",
    "    wac2vec = pickle.load(open(embedding_pickle, 'rb'))\n",
    "else:\n",
    "    with open(embedding_file) as f:\n",
    "        f = f.readlines()\n",
    "        f = [line.split() for line in f]\n",
    "        wac2vec = {line[0]:np.array(line[1:], dtype=np.float64) for line in f}\n",
    "        pickle.dump(wac2vec, open(embedding_pickle, \"wb\" ))\n",
    "\n",
    "len(wac2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4a143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2168990"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concr_scores\n",
    "\n",
    "if os.path.isfile(concreteness_pickle):\n",
    "    concr_scores = pickle.load(open(concreteness_pickle, 'rb'))\n",
    "else:\n",
    "    concr_scores = pd.read_csv(concreteness_file, delimiter='\\t')\n",
    "    concr_scores = concr_scores.dropna()\n",
    "    concr_scores.WORD = concr_scores.WORD.apply(lambda x: x.replace(\"_\", \" \"))\n",
    "    concr_scores.index = concr_scores.WORD\n",
    "    pickle.dump(concr_scores, open(concreteness_pickle, \"wb\" ))\n",
    "\n",
    "len(concr_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc199c",
   "metadata": {},
   "source": [
    "## ConcreteAbstract Class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "001f325f",
   "metadata": {},
   "source": [
    "class ConcreteAbstract:\n",
    "    def __init__(self, word_vectors, concr_scores, word_net):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81683a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e13c006a",
   "metadata": {},
   "source": [
    "### Build Abstraction Tree"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b4a74fb",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "Start with a set of concrete words that have embeddings and are in WordNet.\n",
    " - We'll call these Leaves.  All other embeddings are derived from these.\n",
    "\n",
    "[List of Leaf Words]\n",
    "\n",
    "<Attach embeddings to these words for later use>\n",
    " - We can put these directly into our abstraction tree\n",
    "\n",
    "[Abstraction Tree]\n",
    "\n",
    "<Find a proper Synset for each leaf\n",
    "\n",
    "[List of Leaf Synsets]\n",
    "\n",
    "<Trim leaves that are ancestors of other leaves>\n",
    "\n",
    "[List of True Leaf Synsets]\n",
    "\n",
    "<Make sure only the True Leaf Synsets are in our [Abstraction Tree]>\n",
    "\n",
    "<Create a list of unprocessed Synsets>\n",
    "\n",
    "[Unprocessed Synsets]\n",
    "\n",
    "<Start \"processing\" each of these [Unprocessed Synsets]>\n",
    " <Find a hypernym>\n",
    " <Add hypernym to [Unprocessed Synsets]>\n",
    "  <Track some information>\n",
    "   - list of direct children?\n",
    "   - list of children and their depths?\n",
    "   - number of leaves\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e089c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_abstraction_tree(min_rating=8):\n",
    "\n",
    "    wac_words = list(wac2vec.keys())\n",
    "    wn_words = set(i for i in wn.words())\n",
    "    wn_wac_words = wn_words & set(wac_words)\n",
    "    \n",
    "    concr_scores_subset = concr_scores[concr_scores.RATING >= min_rating]\n",
    "    leaf_words = [w for w in tqdm(wn_wac_words) if w in concr_scores_subset.index]\n",
    "\n",
    "    # Get Leaf Synsets...\n",
    "    leaf_synsets = [wn.synsets(w)[0] for w in leaf_words]\n",
    "\n",
    "    # Initiate Abstraction Tree\n",
    "    embeddings = [wac2vec[w] for w in leaf_words]\n",
    "    data = {\n",
    "        \"SYNSET\" : leaf_synsets,\n",
    "        \"WORD\" : leaf_words,\n",
    "        \"DIST2LEAF\": [0]*len(leaf_synsets),\n",
    "        \"NUM_LEAVES\": [1]*len(leaf_synsets),\n",
    "        \"HYPERNYM\": [[]]*len(leaf_synsets),\n",
    "        \"HYPONYMS\": [[]]*len(leaf_synsets),\n",
    "        \"EMBEDDING\" : embeddings\n",
    "        \n",
    "    }\n",
    "    abstraction_tree = pd.DataFrame(data)\n",
    "    abstraction_tree.index = abstraction_tree.SYNSET\n",
    "\n",
    "    # Get True Leaf Synsets\n",
    "    ancestors = set()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for s in leaf_synsets:\n",
    "            #print(set(s.closure(lambda s: s.hypernyms())))\n",
    "            ancestors = ancestors.union(set(s.closure(lambda s: s.hypernyms())))\n",
    "\n",
    "    # Remove leaves that are ancestors of other leaves\n",
    "    true_leaf_synsets = set(leaf_synsets) - ancestors\n",
    "    ansestor_leaves = set(leaf_synsets) - true_leaf_synsets\n",
    "    abstraction_tree.drop(ansestor_leaves, inplace=True)\n",
    "    \n",
    "    # Remove leaves that have the same Synset\n",
    "    abstraction_tree.drop_duplicates(subset='SYNSET', inplace=True)\n",
    "    \n",
    "    return abstraction_tree"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b4fd206",
   "metadata": {},
   "source": [
    "#abstraction_tree = init_abstraction_tree(3.418)\n",
    "#abstraction_tree = init_abstraction_tree(3.418)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303c7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grow abstraction tree\n",
    "\n",
    "def update_dist2leaf(abstraction_tree, synset, dist):\n",
    "    h_dist = abstraction_tree.loc[synset, 'DIST2LEAF']\n",
    "    if h_dist > dist:\n",
    "        return\n",
    "    \n",
    "    abstraction_tree.at[synset, 'DIST2LEAF'] = dist\n",
    "    \n",
    "    # go up the hypernym chain and set the distances\n",
    "    h = abstraction_tree.loc[synset, 'HYPERNYM']\n",
    "    if len(h) != 0:\n",
    "        update_dist2leaf(abstraction_tree, h[0], dist+1)\n",
    "        \n",
    "\n",
    "def update_num_leaves(abstraction_tree, synset, num_leaves):\n",
    "    orig_num_leaves = abstraction_tree.loc[synset, 'NUM_LEAVES']\n",
    "    abstraction_tree.at[synset, 'NUM_LEAVES'] = orig_num_leaves + num_leaves\n",
    "    \n",
    "    # go up the hypernym chain and update the leaves\n",
    "    h = abstraction_tree.loc[synset, 'HYPERNYM']\n",
    "    if len(h) != 0:\n",
    "        update_num_leaves(abstraction_tree, h[0], num_leaves)\n",
    "\n",
    "def grow_abstraction_tree(abstraction_tree):\n",
    "    \"\"\"Takes an initial abstraction tree (containing only leaves) and grows\n",
    "    the rest of the tree.\"\"\"\n",
    "    \n",
    "    synset_list = list(abstraction_tree['SYNSET'])\n",
    "\n",
    "    # Note: synset_list will grow as we loop through it, so tqdm may reach 100% before it's done\n",
    "    for s in tqdm(synset_list):\n",
    "        h = s.hypernyms()\n",
    "\n",
    "        if len(h) == 0:\n",
    "            continue\n",
    "\n",
    "        h = h[0]\n",
    "\n",
    "        if h not in abstraction_tree.SYNSET:\n",
    "            synset_list.append(h)\n",
    "            abstraction_tree.loc[h] = [\n",
    "                h,    # SYNSET\n",
    "                None, # WORD\n",
    "                0,    # DIST2LEAF\n",
    "                0,    # NUM_LEAVES\n",
    "                [],   # HYPERNYM\n",
    "                [],   # HYPONYMS\n",
    "                None  # EMBEDDING\n",
    "            ]\n",
    "\n",
    "        # Set DIST2LEAF\n",
    "        s_dist = abstraction_tree.loc[s, 'DIST2LEAF']\n",
    "        update_dist2leaf(abstraction_tree, h, s_dist+1)\n",
    "        #h_dist = abstraction_tree.loc[h, 'DIST2LEAF']\n",
    "        #print(s)\n",
    "        #if s_dist >= h_dist:\n",
    "            #while abstraction_tree.loc[h, 'DIST2LEAF']\n",
    "            #abstraction_tree.loc[h, 'DIST2LEAF'] = s_dist + 1\n",
    "\n",
    "        # Set NUM_LEAVES\n",
    "        s_num_leaves = abstraction_tree.loc[s, 'NUM_LEAVES']\n",
    "        update_num_leaves(abstraction_tree, h, s_num_leaves)\n",
    "        #h_num_leaves = abstraction_tree.loc[h, 'NUM_LEAVES']\n",
    "        #abstraction_tree.loc[h, 'NUM_LEAVES'] = h_num_leaves + s_num_leaves\n",
    "\n",
    "        # Add hypernym to synset\n",
    "        abstraction_tree.loc[s, 'HYPERNYM'] = [h]\n",
    "\n",
    "        # Add synset to hypernym\n",
    "        abstraction_tree.loc[h, 'HYPONYMS'].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2757dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display abstraction tree\n",
    "from nltk.tree import Tree\n",
    "\n",
    "display_tree_call_limit = 0\n",
    "   \n",
    "def build_display_tree_safe(df, root_synset, char_limit=20):\n",
    "    \"\"\"Display an abstraction tree starting with the root_synset.\n",
    "    Returns an nltk Tree structure.\n",
    "    Do not use on big trees!\"\"\"\n",
    "    \n",
    "    global display_tree_call_limit\n",
    "    display_tree_call_limit -= 1\n",
    "    \n",
    "    if display_tree_call_limit <= 0:\n",
    "        warnings.warn(\"build_display_tree_safe reached maximum number of calls.\")\n",
    "        return \"MAX\"[:char_limit]\n",
    "    \n",
    "    row = df.loc[root_synset]\n",
    "    root_name = row['SYNSET'].lemmas()[0].name()[:char_limit]\n",
    "    if len(row['HYPONYMS']) == 0:\n",
    "        return root_name\n",
    "    \n",
    "    children = [build_display_tree_safe(df, h, char_limit) for h in row['HYPONYMS']]\n",
    "    \n",
    "    return Tree(root_name, children)\n",
    "\n",
    "\n",
    "def build_display_tree(df, root_synset, char_limit=20, call_limit=500):\n",
    "    global display_tree_call_limit\n",
    "    \n",
    "    display_tree_call_limit = call_limit\n",
    "    return build_display_tree_safe(df, root_synset, char_limit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b10c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d741abbf074d44bb2e66a5461860ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fc7fb57b154ccbacd4c9b0baa68a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"744px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,952.0,744.0\" width=\"952px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">entit</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">physi</text></svg><svg width=\"23.5294%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">matte</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">subst</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">food</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">bever</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cocoa</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">nutri</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">dish</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pizza</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">solid</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">food</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">meat</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">sausa</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">produ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">veget</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">solan</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tomat</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.7647%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"70.5882%\" x=\"23.5294%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">objec</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">whole</text></svg><svg width=\"41.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">artif</text></svg><svg width=\"60%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cover</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">floor</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">rug</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">footw</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">shoe</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cloth</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">dress</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">banda</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">strip</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">band</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">watch</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20%\" x=\"80%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">commo</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">consu</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cloth</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">garme</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">under</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">brass</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.8333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"58.3333%\" x=\"41.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">livin</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">organ</text></svg><svg width=\"85.7143%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">anima</text></svg><svg width=\"16.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">inver</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">arthr</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">insec</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">beetl</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.33333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"83.3333%\" x=\"16.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">chord</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">verte</text></svg><svg width=\"20%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">amphi</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">frog</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20%\" x=\"20%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">repti</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">diaps</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">croco</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">croco</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20%\" x=\"40%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">aquat</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">fish</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">carti</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">elasm</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">shark</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">mamma</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">place</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ungul</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">even-</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">swine</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">hog</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">prima</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ape</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">anthr</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">great</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">goril</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"14.2857%\" x=\"85.7143%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">plant</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">vascu</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">herb</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">grami</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">grass</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cerea</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"92.8571%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.8333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.8235%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.88235%\" x=\"94.1176%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">thing</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">part</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">body_</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tissu</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">anima</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">flap</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">prote</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">eyeli</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.0588%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('entit', [Tree('physi', [Tree('matte', [Tree('subst', [Tree('food', [Tree('bever', ['cocoa']), Tree('nutri', [Tree('dish', ['pizza'])])])]), Tree('solid', [Tree('food', [Tree('meat', ['sausa']), Tree('produ', [Tree('veget', [Tree('solan', ['tomat'])])])])])]), Tree('objec', [Tree('whole', [Tree('artif', [Tree('cover', [Tree('floor', ['rug']), Tree('footw', ['shoe']), Tree('cloth', [Tree('dress', ['banda'])])]), Tree('strip', [Tree('band', ['watch'])]), Tree('commo', [Tree('consu', [Tree('cloth', [Tree('garme', [Tree('under', ['brass'])])])])])]), Tree('livin', [Tree('organ', [Tree('anima', [Tree('inver', [Tree('arthr', [Tree('insec', ['beetl'])])]), Tree('chord', [Tree('verte', [Tree('amphi', ['frog']), Tree('repti', [Tree('diaps', [Tree('croco', ['croco'])])]), Tree('aquat', [Tree('fish', [Tree('carti', [Tree('elasm', ['shark'])])])]), Tree('mamma', [Tree('place', [Tree('ungul', [Tree('even-', [Tree('swine', ['hog'])])]), Tree('prima', [Tree('ape', [Tree('anthr', [Tree('great', ['goril'])])])])])])])])]), Tree('plant', [Tree('vascu', [Tree('herb', [Tree('grami', [Tree('grass', ['cerea'])])])])])])])])]), Tree('thing', [Tree('part', [Tree('body_', [Tree('tissu', [Tree('anima', [Tree('flap', [Tree('prote', ['eyeli'])])])])])])])])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_tree = init_abstraction_tree(9)\n",
    "grow_abstraction_tree(tmp_tree)\n",
    "build_display_tree(tmp_tree, wn.synset('entity.n.01'), char_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abbbce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec10b3dd5d94cbcaaee627045730b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abstraction_tree = init_abstraction_tree(concreteness_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46691939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180ccafa3ac84757a249fb3e503e2829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grow_abstraction_tree(abstraction_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe51cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12043f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Positive Synsets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4677347",
   "metadata": {},
   "source": [
    "Concrete ideas (synsets) already have embeddings.  We want to get embeddings for more abstract ideas.\n",
    "\n",
    "We will try to add a classifier to each abstract idea (synset).\n",
    "\n",
    "Every classifier will need a set of positive embeddings to train on.\n",
    "\n",
    "These embeddings will come from the hyponym synsets.\n",
    "\n",
    "If there are not enough synsets under a hypernym, then it will not get a classifier.\n",
    "\n",
    "\n",
    "We'll want to do a breadth first search...\n",
    "We can use NUM_LEAVES to tell us if a particular synset will have a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623286dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leaf(synset):\n",
    "    if synset not in abstraction_tree.index:\n",
    "        return False\n",
    "    return abstraction_tree.loc[synset, 'DIST2LEAF'] == 0\n",
    "\n",
    "def classifier_capable(synset):\n",
    "    return abstraction_tree.loc[synset, 'NUM_LEAVES'] >= pos_count\n",
    "\n",
    "def is_embedding_capable(synset):\n",
    "    \"\"\"Return true if the synset is capable of having an embedding.\"\"\"\n",
    "    if synset not in abstraction_tree.index:\n",
    "        return False\n",
    "    return is_leaf(synset) or classifier_capable(synset)\n",
    "\n",
    "# dist2leaf gives us the furthest leaf.\n",
    "# This can be used as a \"sort\" of way to determine abstraction level\n",
    "def dist2leaf(synset):\n",
    "    return abstraction_tree.loc[synset, 'DIST2LEAF']\n",
    "\n",
    "def is_closer_to_leaf(synset, dist):\n",
    "    \"\"\"Return true if the synset is closer to a leaf than dist.\"\"\"\n",
    "    return dist2leaf(synset) < dist\n",
    "\n",
    "def get_hyponyms(synset):\n",
    "    \"\"\"Return a list of hyponyms, or itself if there are none.\"\"\"\n",
    "    if synset not in abstraction_tree.index:\n",
    "        return None\n",
    "    hypos = abstraction_tree.loc[synset, 'HYPONYMS']\n",
    "    if len(hypos) == 0:\n",
    "        return [synset]\n",
    "    else:\n",
    "        return hypos\n",
    "\n",
    "def count_embedding_capable(synset_list):\n",
    "    \"\"\"Given a list of synsets, returns a count of how many are capable of having an embedding.\"\"\"\n",
    "    return sum((is_embedding_capable(s) == True)*1 for s in synset_list)\n",
    "\n",
    "def expand_hyponym_list(synset_list):\n",
    "    hypos = []\n",
    "    for s in synset_list:\n",
    "        hypos += get_hyponyms(s)\n",
    "    return hypos\n",
    "\n",
    "def remove_embedding_incapable(synset_list):\n",
    "    synset_list = np.array(synset_list)\n",
    "    return list(synset_list[list(map(is_embedding_capable, synset_list))])\n",
    "\n",
    "def find_positive_examples(synset, depth=100):\n",
    "    pos = get_hyponyms(synset)\n",
    "    for _ in range(depth):\n",
    "        pos = expand_hyponym_list(pos)\n",
    "        if count_embedding_capable(pos) >= pos_count:\n",
    "            return remove_embedding_incapable(pos)\n",
    "    \n",
    "    raise Exception(\"Reached depth of {} without finding enough positive example: {}\".format(depth, synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f22cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_positive_examples(wn.synset('artifact.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e7b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21851446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find negative examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b6539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_negative_examples(synset, pos_examples):\n",
    "    # All synsets\n",
    "    neg = np.array(abstraction_tree['SYNSET'])\n",
    "    # Embedding capable synsets\n",
    "    neg = neg[list(map(is_embedding_capable, neg))]\n",
    "    # Use only more concrete words (words that are closer to a leaf)\n",
    "    dist2leaf = abstraction_tree.loc[synset, 'DIST2LEAF']\n",
    "    neg = neg[list(map(lambda x: is_closer_to_leaf(x, dist2leaf), neg))]\n",
    "    # neg examples not in positive examples\n",
    "    neg = set(neg) - set(pos_examples)\n",
    "    # Don't include yourself\n",
    "    #neg.remove(synset) # removed by the closer check\n",
    "    neg_examples = random.sample(list(neg), k=min(neg_count, len(neg)))\n",
    "    return neg_examples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38d8edb3",
   "metadata": {},
   "source": [
    "pos_examples = find_positive_examples(wn.synset('entity.n.01'))\n",
    "neg_examples = find_negative_examples(wn.synset('entity.n.01'), pos_examples)\n",
    "neg_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db4f744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positive and negative examples for a single hypernym\n",
    "def add_positive_negative_examples(synset):\n",
    "    pos = find_positive_examples(synset)\n",
    "    abstraction_tree.at[synset, 'POSITIVE'] = pos\n",
    "    neg = find_negative_examples(synset, pos)\n",
    "    abstraction_tree.at[synset, 'NEGATIVE'] = neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e533b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_capable():\n",
    "    \"\"\"Get a list of synsets capable of having a classifier.\"\"\"\n",
    "    return [s for s in abstraction_tree['SYNSET'] if classifier_capable(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59c8ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positive and negative examples for all hypernyms\n",
    "def add_pos_neg_all():\n",
    "    \"\"\"Add positive and negative examples for each calssifier capable synset.\"\"\"\n",
    "    abstraction_tree['POSITIVE'] = [[]]*len(abstraction_tree)\n",
    "    abstraction_tree['NEGATIVE'] = [[]]*len(abstraction_tree)\n",
    "    for s in tqdm(get_classifier_capable()):\n",
    "        add_positive_negative_examples(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b6d763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24edcbab1c9417caa6c23daaabf42d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "add_pos_neg_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "914a3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Train/Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546a0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test(synset):\n",
    "    pos_examples = find_positive_examples(synset)\n",
    "    neg_examples = find_negative_examples(synset, pos_examples)\n",
    "    X = pos_examples + neg_examples\n",
    "    y = list(np.ones(len(pos_examples))) + list(np.zeros(len(neg_examples)))\n",
    "    return train_test_split(X, y, test_size=test_pct, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "606412c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_out_train_test():\n",
    "    abstraction_tree['X_TRAIN'] = [[]]*len(abstraction_tree)\n",
    "    abstraction_tree['X_TEST']  = [[]]*len(abstraction_tree)\n",
    "    abstraction_tree['Y_TRAIN'] = [[]]*len(abstraction_tree)\n",
    "    abstraction_tree['Y_TEST']  = [[]]*len(abstraction_tree)\n",
    "    \n",
    "    synsets = get_classifier_capable()\n",
    "    for s in tqdm(synsets):\n",
    "        X_train, X_test, y_train, y_test  = build_train_test(s)\n",
    "        abstraction_tree.at[s, 'X_TRAIN'] = X_train\n",
    "        abstraction_tree.at[s, 'X_TEST']  = X_test\n",
    "        abstraction_tree.at[s, 'Y_TRAIN'] = y_train\n",
    "        abstraction_tree.at[s, 'Y_TEST']  = y_test        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33f5387b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bae099fa7d64507acb1fad8f673de2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fill_out_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3708ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1c1b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afe4e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82332572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefs(classifier):\n",
    "    return classifier.coef_[0]\n",
    "\n",
    "def build_classifiers():\n",
    "    abstraction_tree['CLASSIFIER']  = [None]*len(abstraction_tree)\n",
    "    cc = get_classifier_capable()\n",
    "    # Start with synsets close to leaves and work our way up to more abstract hypernyms\n",
    "    cc.sort(key=dist2leaf)\n",
    "    for ss in tqdm(cc):\n",
    "        #print(ss)\n",
    "        X_train = abstraction_tree.loc[ss, 'X_TRAIN']\n",
    "        y_train = abstraction_tree.loc[ss, 'Y_TRAIN']\n",
    "        X_train = list(abstraction_tree.loc[X_train, 'EMBEDDING'])\n",
    "        \n",
    "        # NOTE: Remove Me\n",
    "        \"\"\"\n",
    "        neg_count = 0\n",
    "        for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "            if y == 0:\n",
    "                X_train[i] = [0.5]*513\n",
    "                neg_count += 1\n",
    "                #X_train.append([-1]*512)\n",
    "        X_train += [[-0.5]*513]*neg_count\n",
    "        y_train += [0]*neg_count\n",
    "        \"\"\"\n",
    "                \n",
    "        #lr = LogisticRegression()\n",
    "        #lr = LogisticRegression(penalty='l2', solver='lbfgs', C=0.25)\n",
    "        lr = LogisticRegression(C=0.25, max_iter=1000)\n",
    "        lr.fit(X_train, y_train)\n",
    "        abstraction_tree.at[ss, 'CLASSIFIER'] = lr\n",
    "        abstraction_tree.at[ss, 'EMBEDDING'] = coefs(lr)\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8ebc741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5bbfa8fd5040c1b08faa948b355cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67387e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstraction_tree.loc[get_classifier_capable(), 'EMBEDDING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e51e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7246a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (vs negative examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2b609b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Baseline (vs negative examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8b80e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_random_baseline():\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "afd3f4c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_random_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b35fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Baseline (vs negative examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f49b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_most_common_baseline():\n",
    "    pos, tot = 0, 0\n",
    "    for ss in get_classifier_capable():\n",
    "        y = abstraction_tree.loc[ss, 'Y_TRAIN']\n",
    "        pos += sum(y)\n",
    "        tot += len(y)\n",
    "        y = abstraction_tree.loc[ss, 'Y_TEST']\n",
    "        pos += sum(y)\n",
    "        tot += len(y)\n",
    "    return max(pos/tot, 1-(pos/tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "71d35ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5853658536585367"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_most_common_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dd564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3931201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifiers():\n",
    "    golds = []\n",
    "    preds = []\n",
    "    cc = get_classifier_capable()\n",
    "    # Evaluate in order (Not stricly needed, but helpful)\n",
    "    cc.sort(key=dist2leaf)\n",
    "    for ss in cc:\n",
    "        X_test = abstraction_tree.loc[ss, 'X_TEST']\n",
    "        y_test = abstraction_tree.loc[ss, 'Y_TEST']\n",
    "        X_test = list(abstraction_tree.loc[X_test, 'EMBEDDING'])\n",
    "        \n",
    "        c = abstraction_tree.loc[ss, 'CLASSIFIER']\n",
    "        pred = c.predict_proba(X_test)\n",
    "        #pred = lr.predict_proba(X_test)\n",
    "        pred = list(np.argmax(pred, axis=1))\n",
    "        \"\"\"\n",
    "        print(ss)\n",
    "        print(list(map(int, y_test)))\n",
    "        print(pred)\n",
    "        print(metrics.accuracy_score(y_test, pred))\n",
    "        print()\n",
    "        \"\"\"\n",
    "        preds = preds + pred\n",
    "        golds = golds + y_test\n",
    "    return metrics.accuracy_score(golds, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "292fd34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7252100840336134"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d9d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3917fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (vs other classifiers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "725c5747",
   "metadata": {},
   "source": [
    "# 7 - 66\n",
    "# 6 - 115\n",
    "# 5 - 161\n",
    "# 5 - 241 (10, 20)\n",
    "len(get_classifier_capable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af323fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classifiers(abstraction_tree):\n",
    "    classifiers = dd(None)\n",
    "    for ss in get_classifier_capable():\n",
    "        classf = abstraction_tree.loc[ss, 'CLASSIFIER']\n",
    "        classifiers[ss] = classf\n",
    "\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "566ed22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier_single(classifiers, x_ss, gold_ss, num_distractors=5):\n",
    "    #gold_ss = wn.synset('conveyance.n.03')\n",
    "    vec = abstraction_tree.loc[x_ss, 'EMBEDDING']\n",
    "    probs = [(gold_ss, classifiers[gold_ss].predict_proba([vec])[0][1])]\n",
    "    distractors=[]\n",
    "    for _ in range(num_distractors):\n",
    "        ss = random.choice(list(classifiers.keys()))\n",
    "        if x_ss in abstraction_tree.loc[ss, 'POSITIVE']:\n",
    "            continue\n",
    "        if ss in distractors:\n",
    "            continue\n",
    "        distractors.append(ss)\n",
    "        probs.append((ss, classifiers[ss].predict_proba([vec])[0][1]))\n",
    "    best = max(probs, key=itemgetter(1))\n",
    "    #print(x_ss, best[0], gold_ss)\n",
    "    score = (best[0] == gold_ss)*1\n",
    "    num_possibilities = len(probs) # To help calculate baseline\n",
    "    return score, num_possibilities"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b6e913c",
   "metadata": {},
   "source": [
    "test_classifier_single(x_ss, gold_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7ff4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier_full(classifiers, gold_ss, num_distractors=5):\n",
    "    test = abstraction_tree.loc[gold_ss, 'X_TEST']\n",
    "    pos = abstraction_tree.loc[gold_ss, 'POSITIVE']\n",
    "    X = set(test).intersection(pos)\n",
    "    total_score = 0\n",
    "    total_possibilities = 0\n",
    "    total_tests = len(X)\n",
    "    for x in X:\n",
    "        score, num_possibilities = test_classifier_single(classifiers, x, gold_ss, num_distractors)\n",
    "        total_score += score\n",
    "        total_possibilities += num_possibilities\n",
    "    return total_score, total_tests, total_possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "957bb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gold_ss = wn.synset('mammal.n.01')\n",
    "#test_classifier_full(gold_ss, num_distractors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cef232b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/evaluate all classifiers\n",
    "def evaluate_all_classifiers(classifiers, num_distractors=10):\n",
    "    grand_total_score, grand_total_tests, grand_total_possibilities = 0,0,0\n",
    "    for gold_ss in tqdm(classifiers.keys()):\n",
    "        #print(gold_ss)\n",
    "        total_score, total_tests, total_possibilities = test_classifier_full(classifiers, gold_ss, num_distractors)\n",
    "        #print(total_score, total_tests, total_possibilities)\n",
    "        grand_total_score += total_score\n",
    "        grand_total_tests += total_tests\n",
    "        grand_total_possibilities += total_possibilities\n",
    "\n",
    "    score = grand_total_score/grand_total_tests\n",
    "    baseline = grand_total_tests/grand_total_possibilities\n",
    "\n",
    "    return score, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd97108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = get_all_classifiers(abstraction_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c6cb146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3533fb6cf6f48008ddddc2747e24bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:     0.3760504201680672\n",
      "baseline:  0.09663012586276898\n"
     ]
    }
   ],
   "source": [
    "score, baseline = evaluate_all_classifiers(classifiers, 10)\n",
    "print(\"score:    \", score)\n",
    "print(\"baseline: \", baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d4cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5db271c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Classifiers\n",
    "len(get_classifier_capable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8b286b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Lemmas\n",
    "sum(len(ss.lemmas()) for ss in get_classifier_capable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a566944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "200bf564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 2D (abandoned)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7750de77",
   "metadata": {},
   "source": [
    "https://stackabuse.com/dimensionality-reduction-in-python-with-scikit-learn/\n",
    "https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c\n",
    " - maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5e59ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_get_categories(start_ss, depth=1):\n",
    "    hypos = abstraction_tree.loc[start_ss, 'HYPONYMS']\n",
    "    if depth <= 0:\n",
    "        return hypos\n",
    "    \n",
    "    ret = []\n",
    "    for h in hypos:\n",
    "        ret = ret + visual_get_categories(h, depth-1)\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16485a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offspring(parent_ss):\n",
    "    offspring = []\n",
    "    absraction_levels = []\n",
    "    hypos = abstraction_tree.loc[parent_ss, 'HYPONYMS']\n",
    "    emb = abstraction_tree.loc[parent_ss, 'EMBEDDING']\n",
    "    a_level = abstraction_tree.loc[parent_ss, 'DIST2LEAF']\n",
    "\n",
    "    #print(parent_ss)\n",
    "    #print(emb)\n",
    "    #print(hypos)\n",
    "    if emb is not None:\n",
    "        offspring.append(emb)\n",
    "        absraction_levels.append(a_level)\n",
    "    for ss in hypos:\n",
    "        off, a_level = get_offspring(ss)\n",
    "        offspring += off\n",
    "        absraction_levels += a_level\n",
    "    return offspring, absraction_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f82f0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_get_X_features(ss_list):\n",
    "    X_features = []\n",
    "    X_colors = []\n",
    "    for i, parent_ss in enumerate(ss_list):\n",
    "        #print(i)\n",
    "        offspring, a_levels = get_offspring(parent_ss)\n",
    "        X_features += offspring\n",
    "        X_colors += [i]*len(offspring)\n",
    "        #X_colors += a_levels\n",
    "    \n",
    "    return X_features, X_colors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f49e51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_categories = visual_get_categories(wn.synset('entity.n.01'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3cf21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features, X_colors = visual_get_X_features(visual_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e07b40f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1278"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41c7e78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1278"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e940c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8a1b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "pca = PCA()\n",
    "pca.fit_transform(X_features)\n",
    "pca_variance = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e637dfa",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(513), pca_variance, alpha=0.5, align='center', label='individual variance')\n",
    "plt.legend()\n",
    "plt.ylabel('Variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad1ecfd4",
   "metadata": {},
   "source": [
    "pca2 = PCA(n_components=368)\n",
    "pca2.fit(X_features)\n",
    "x_3d = pca2.transform(X_features)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_3d[:,0], x_3d[:,105], c=X_colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d77a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "756b2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereate Embedding File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82313007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c6e3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = deepcopy(wac2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0583798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'tree' in new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d054c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_embeddings():\n",
    "    new_embeddings = deepcopy(wac2vec)\n",
    "    cc = get_classifier_capable()\n",
    "    # Start with synsets close to leaves and work our way up to more abstract hypernyms\n",
    "    # (PFI) Should this be reversed maybe?\n",
    "    cc.sort(key=dist2leaf)\n",
    "    processed_lemmas=[]\n",
    "    count = 0\n",
    "    for ss in cc:\n",
    "        for lemma in ss.lemmas():\n",
    "            name = lemma.name()\n",
    "            if name in new_embeddings:\n",
    "                if name in processed_lemmas:\n",
    "                    #print(\"Duplicate:\", name)\n",
    "                    pass\n",
    "                else:\n",
    "                    count += 1\n",
    "                    processed_lemmas.append(name)\n",
    "                    # On duplicate, use the more concrete lemma\n",
    "                    #new_embeddings[name] = abstraction_tree.loc[ss, 'EMBEDDING']\n",
    "                # On duplcate, use the more abstract lemma\n",
    "                new_embeddings[name] = abstraction_tree.loc[ss, 'EMBEDDING']\n",
    "                #print(' '.join(str(x) for x in abstraction_tree.loc[ss, 'EMBEDDING']))\n",
    "                \n",
    "    return new_embeddings, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "94b7b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings, count = get_new_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d488334",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings_base_path = \"ddata/new_embeddings\"\n",
    "new_embeddings_path = \"{}_th{}_p{}_n{}_t{}_c{}.txt\".format(new_embeddings_base_path, concreteness_threshold, pos_count, neg_count, test_pct, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b733a504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ddata/new_embeddings_th7_p10_n20_t0.33_c99.txt'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2dbcbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_embeddings_path, 'w') as f:\n",
    "    for key in list(new_embeddings.keys()):\n",
    "        f.write(\"{} {}\\n\".format(key, ' '.join(str(x) for x in new_embeddings[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b4e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
