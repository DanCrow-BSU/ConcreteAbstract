{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b17096e2",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "To create a set of abstrct word embeddings from a set of concrete word embeddings.\n",
    "\n",
    "Strategy:\n",
    "We have three datasets we'll be using:\n",
    "[wac2vec]\n",
    "[WordNet]\n",
    "[Concretness Ratings]\n",
    "\n",
    "\n",
    "Let's create a class to store these and hold functions for manipulating them:\n",
    "[ConcreteAbstract]\n",
    " - isa: class\n",
    " - anchor\n",
    "\n",
    "[AbstractionTree]\n",
    " - anchor\n",
    " - a \"tree\"\n",
    "  - each leaf has a concrete word, embedding, abstraction level\n",
    "  - every parent is an abstract word one (or more?) abstraction level(s) above its children\n",
    "   - contains a classifier that can classify its children\n",
    " - pandas dataframe\n",
    "  - I think pandas will have some useful functions\n",
    "   \n",
    "\n",
    "What are some possible functions we may need?\n",
    "init_abstraction_tree()\n",
    " - takes the three datasets and starts a new tree with the root\n",
    "\n",
    "build_abstraction_tree()\n",
    " - takes the three datasets and builds the [AbstractionTree]\n",
    "  - starts with the leaves and builds towards the root(s)\n",
    " - we can try different kinds of trees using variations of this function\n",
    "\n",
    "\n",
    "build_classifiers()\n",
    " - takes an [AbstractionTree] and builds classifieres for the abstract words\n",
    "\n",
    "\n",
    "\n",
    "Wrapper classes for data?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5f6ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/crow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pickle\n",
    "#import sklearn\n",
    "#from sklearn import metrics\n",
    "#from sklearn import linear_model\n",
    "#import random\n",
    "#import numpy as np\n",
    "#from wac import WAC\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "#from sklearn import neural_network\n",
    "#import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict as dd\n",
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77be4a",
   "metadata": {},
   "source": [
    "## Initiate Datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4e799d3",
   "metadata": {},
   "source": [
    "f = open('ddata/clip.bertvocab.embeddings.513.txt').readlines()\n",
    "f = [line.split() for line in f]\n",
    "wac2vec = {line[0]:np.array(line[1:], dtype=np.float32) for line in f}\n",
    "pickle.dump(wac2vec, open( \"ddata/clip.bertvocab.embeddings.513.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5190fc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wac2vec = pickle.load(open('ddata/clip.bertvocab.embeddings.513.pkl', 'rb'))\n",
    "len(wac2vec)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a5a9aaf",
   "metadata": {},
   "source": [
    "concr_scores = pd.read_csv('ddata/AC_ratings_google3m_koeper_SiW.csv', delimiter='\\t')\n",
    "concr_scores = concr_scores.dropna()\n",
    "concr_scores.WORD = concr_scores.WORD.apply(lambda x: x.replace(\"_\", \" \"))\n",
    "concr_scores.index = concr_scores.WORD\n",
    "pickle.dump(concr_scores, open( \"ddata/AC_ratings_google3m_koeper_SiW.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44bd6152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2168990"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concr_scores = pickle.load(open('ddata/AC_ratings_google3m_koeper_SiW.pkl', 'rb'))\n",
    "len(concr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "978888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wac_words = list(wac2vec.keys())\n",
    "wn_words = set(i for i in wn.words())\n",
    "wn_wac_words = wn_words & set(wac_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f155b51",
   "metadata": {},
   "source": [
    "## ConcreteAbstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc89222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteAbstract:\n",
    "    def __init__(self, word_vectors, concr_scores, word_net):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106c434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b28d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cbee9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cow.n.01'),\n",
       " Synset('cow.n.02'),\n",
       " Synset('cow.n.03'),\n",
       " Synset('overawe.v.01')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('cow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad4d99cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method synsets in module nltk.corpus.reader.wordnet:\n",
      "\n",
      "synsets(lemma, pos=None, lang='eng', check_exceptions=True) method of nltk.corpus.reader.wordnet.WordNetCorpusReader instance\n",
      "    Load all synsets with a given lemma and part of speech tag.\n",
      "    If no pos is specified, all synsets for all parts of speech\n",
      "    will be loaded.\n",
      "    If lang is specified, all the synsets associated with the lemma name\n",
      "    of that language will be returned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wn.synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4443eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synset = wn.synsets('cow')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a756b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cow.n.02')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d40801bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Synset in module nltk.corpus.reader.wordnet object:\n",
      "\n",
      "class Synset(_WordNetObject)\n",
      " |  Synset(wordnet_corpus_reader)\n",
      " |  \n",
      " |  Create a Synset from a \"<lemma>.<pos>.<number>\" string where:\n",
      " |  <lemma> is the word's morphological stem\n",
      " |  <pos> is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\n",
      " |  <number> is the sense number, counting from 0.\n",
      " |  \n",
      " |  Synset attributes, accessible via methods with the same name:\n",
      " |  \n",
      " |  - name: The canonical name of this synset, formed using the first lemma\n",
      " |    of this synset. Note that this may be different from the name\n",
      " |    passed to the constructor if that string used a different lemma to\n",
      " |    identify the synset.\n",
      " |  - pos: The synset's part of speech, matching one of the module level\n",
      " |    attributes ADJ, ADJ_SAT, ADV, NOUN or VERB.\n",
      " |  - lemmas: A list of the Lemma objects for this synset.\n",
      " |  - definition: The definition for this synset.\n",
      " |  - examples: A list of example strings for this synset.\n",
      " |  - offset: The offset in the WordNet dict file of this synset.\n",
      " |  - lexname: The name of the lexicographer file containing this synset.\n",
      " |  \n",
      " |  Synset methods:\n",
      " |  \n",
      " |  Synsets have the following methods for retrieving related Synsets.\n",
      " |  They correspond to the names for the pointer symbols defined here:\n",
      " |  https://wordnet.princeton.edu/documentation/wninput5wn\n",
      " |  These methods all return lists of Synsets.\n",
      " |  \n",
      " |  - hypernyms, instance_hypernyms\n",
      " |  - hyponyms, instance_hyponyms\n",
      " |  - member_holonyms, substance_holonyms, part_holonyms\n",
      " |  - member_meronyms, substance_meronyms, part_meronyms\n",
      " |  - attributes\n",
      " |  - entailments\n",
      " |  - causes\n",
      " |  - also_sees\n",
      " |  - verb_groups\n",
      " |  - similar_tos\n",
      " |  \n",
      " |  Additionally, Synsets support the following methods specific to the\n",
      " |  hypernym relation:\n",
      " |  \n",
      " |  - root_hypernyms\n",
      " |  - common_hypernyms\n",
      " |  - lowest_common_hypernyms\n",
      " |  \n",
      " |  Note that Synsets do not support the following relations because\n",
      " |  these are defined by WordNet as lexical relations:\n",
      " |  \n",
      " |  - antonyms\n",
      " |  - derivationally_related_forms\n",
      " |  - pertainyms\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Synset\n",
      " |      _WordNetObject\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, wordnet_corpus_reader)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  acyclic_tree = acyclic_depth_first(tree, children=<built-in function iter>, depth=-1, cut_mark=None, traversed=None)\n",
      " |      Traverse the nodes of a tree in depth-first order,\n",
      " |      discarding eventual cycles within any branch,\n",
      " |      adding cut_mark (when specified) if cycles were truncated.\n",
      " |      \n",
      " |      The first argument should be the tree root;\n",
      " |      children should be a function taking as argument a tree node\n",
      " |      and returning an iterator of the node's children.\n",
      " |      \n",
      " |      Catches all cycles:\n",
      " |      \n",
      " |      >>> import nltk\n",
      " |      >>> from nltk.util import acyclic_depth_first as acyclic_tree\n",
      " |      >>> wn=nltk.corpus.wordnet\n",
      " |      >>> from pprint import pprint\n",
      " |      >>> pprint(acyclic_tree(wn.synset('dog.n.01'), lambda s:s.hypernyms(),cut_mark='...'))\n",
      " |      [Synset('dog.n.01'),\n",
      " |       [Synset('canine.n.02'),\n",
      " |        [Synset('carnivore.n.01'),\n",
      " |         [Synset('placental.n.01'),\n",
      " |          [Synset('mammal.n.01'),\n",
      " |           [Synset('vertebrate.n.01'),\n",
      " |            [Synset('chordate.n.01'),\n",
      " |             [Synset('animal.n.01'),\n",
      " |              [Synset('organism.n.01'),\n",
      " |               [Synset('living_thing.n.01'),\n",
      " |                [Synset('whole.n.02'),\n",
      " |                 [Synset('object.n.01'),\n",
      " |                  [Synset('physical_entity.n.01'),\n",
      " |                   [Synset('entity.n.01')]]]]]]]]]]]]],\n",
      " |       [Synset('domestic_animal.n.01'), \"Cycle(Synset('animal.n.01'),-3,...)\"]]\n",
      " |  \n",
      " |  closure(self, rel, depth=-1)\n",
      " |      Return the transitive closure of source under the rel\n",
      " |      relationship, breadth-first, discarding cycles:\n",
      " |      \n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> computer = wn.synset('computer.n.01')\n",
      " |      >>> topic = lambda s:s.topic_domains()\n",
      " |      >>> print(list(computer.closure(topic)))\n",
      " |      [Synset('computer_science.n.01')]\n",
      " |      \n",
      " |      UserWarning: Discarded redundant search for Synset('computer.n.01') at depth 2\n",
      " |      \n",
      " |      \n",
      " |      Include redundant paths (but only once), avoiding duplicate searches\n",
      " |      (from 'animal.n.01' to 'entity.n.01'):\n",
      " |      \n",
      " |      >>> dog = wn.synset('dog.n.01')\n",
      " |      >>> hyp = lambda s:s.hypernyms()\n",
      " |      >>> print(list(dog.closure(hyp)))\n",
      " |      [Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'), Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'), Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'), Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'), Synset('physical_entity.n.01'), Synset('entity.n.01')]\n",
      " |      \n",
      " |      UserWarning: Discarded redundant search for Synset('animal.n.01') at depth 7\n",
      " |  \n",
      " |  common_hypernyms(self, other)\n",
      " |      Find all synsets that are hypernyms of this synset and the\n",
      " |      other synset.\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: other input synset.\n",
      " |      :return: The synsets that are hypernyms of both synsets.\n",
      " |  \n",
      " |  definition(self, lang='eng')\n",
      " |      Return definition in specified language\n",
      " |  \n",
      " |  examples(self, lang='eng')\n",
      " |      Return examples in specified language\n",
      " |  \n",
      " |  frame_ids(self)\n",
      " |  \n",
      " |  hypernym_distances(self, distance=0, simulate_root=False)\n",
      " |      Get the path(s) from this synset to the root, counting the distance\n",
      " |      of each node from the initial node on the way. A set of\n",
      " |      (synset, distance) tuples is returned.\n",
      " |      \n",
      " |      :type distance: int\n",
      " |      :param distance: the distance (number of edges) from this hypernym to\n",
      " |          the original hypernym ``Synset`` on which this method was called.\n",
      " |      :return: A set of ``(Synset, int)`` tuples where each ``Synset`` is\n",
      " |         a hypernym of the first ``Synset``.\n",
      " |  \n",
      " |  hypernym_paths(self)\n",
      " |      Get the path(s) from this synset to the root, where each path is a\n",
      " |      list of the synset nodes traversed on the way to the root.\n",
      " |      \n",
      " |      :return: A list of lists, where each list gives the node sequence\n",
      " |         connecting the initial ``Synset`` node and a root node.\n",
      " |  \n",
      " |  jcn_similarity(self, other, ic, verbose=False)\n",
      " |      Jiang-Conrath Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node) and that of the two input Synsets. The relationship is\n",
      " |      given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)).\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type  ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects.\n",
      " |  \n",
      " |  lch_similarity(self, other, verbose=False, simulate_root=True)\n",
      " |      Leacock Chodorow Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      shortest path that connects the senses (as above) and the maximum depth\n",
      " |      of the taxonomy in which the senses occur. The relationship is given as\n",
      " |      -log(p/2d) where p is the shortest path length and d is the taxonomy\n",
      " |      depth.\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      " |          normally greater than 0. None is returned if no connecting path\n",
      " |          could be found. If a ``Synset`` is compared with itself, the\n",
      " |          maximum score is returned, which varies depending on the taxonomy\n",
      " |          depth.\n",
      " |  \n",
      " |  lemma_names(self, lang='eng')\n",
      " |      Return all the lemma_names associated with the synset\n",
      " |  \n",
      " |  lemmas(self, lang='eng')\n",
      " |      Return all the lemma objects associated with the synset\n",
      " |  \n",
      " |  lexname(self)\n",
      " |  \n",
      " |  lin_similarity(self, other, ic, verbose=False)\n",
      " |      Lin Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node) and that of the two input Synsets. The relationship is\n",
      " |      given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects, in the range 0 to 1.\n",
      " |  \n",
      " |  lowest_common_hypernyms(self, other, simulate_root=False, use_min_depth=False)\n",
      " |      Get a list of lowest synset(s) that both synsets have as a hypernym.\n",
      " |      When `use_min_depth == False` this means that the synset which appears\n",
      " |      as a hypernym of both `self` and `other` with the lowest maximum depth\n",
      " |      is returned or if there are multiple such synsets at the same depth\n",
      " |      they are all returned\n",
      " |      \n",
      " |      However, if `use_min_depth == True` then the synset(s) which has/have\n",
      " |      the lowest minimum depth and appear(s) in both paths is/are returned.\n",
      " |      \n",
      " |      By setting the use_min_depth flag to True, the behavior of NLTK2 can be\n",
      " |      preserved. This was changed in NLTK3 to give more accurate results in a\n",
      " |      small set of cases, generally with synsets concerning people. (eg:\n",
      " |      'chef.n.01', 'fireman.n.01', etc.)\n",
      " |      \n",
      " |      This method is an implementation of Ted Pedersen's \"Lowest Common\n",
      " |      Subsumer\" method from the Perl Wordnet module. It can return either\n",
      " |      \"self\" or \"other\" if they are a hypernym of the other.\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: other input synset\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (False by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to True to enable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will need to be added\n",
      " |          for nouns as well.\n",
      " |      :type use_min_depth: bool\n",
      " |      :param use_min_depth: This setting mimics older (v2) behavior of NLTK\n",
      " |          wordnet If True, will use the min_depth function to calculate the\n",
      " |          lowest common hypernyms. This is known to give strange results for\n",
      " |          some synset pairs (eg: 'chef.n.01', 'fireman.n.01') but is retained\n",
      " |          for backwards compatibility\n",
      " |      :return: The synsets that are the lowest common hypernyms of both\n",
      " |          synsets\n",
      " |  \n",
      " |  max_depth(self)\n",
      " |      :return: The length of the longest hypernym path from this\n",
      " |          synset to the root.\n",
      " |  \n",
      " |  min_depth(self)\n",
      " |      :return: The length of the shortest hypernym path from this\n",
      " |          synset to the root.\n",
      " |  \n",
      " |  mst = unweighted_minimum_spanning_tree(tree, children=<built-in function iter>)\n",
      " |      Output a Minimum Spanning Tree (MST) of an unweighted graph,\n",
      " |      by traversing the nodes of a tree in breadth-first order,\n",
      " |      discarding eventual cycles.\n",
      " |      \n",
      " |      The first argument should be the tree root;\n",
      " |      children should be a function taking as argument a tree node\n",
      " |      and returning an iterator of the node's children.\n",
      " |      \n",
      " |      >>> import nltk\n",
      " |      >>> from nltk.util import unweighted_minimum_spanning_tree as mst\n",
      " |      >>> wn=nltk.corpus.wordnet\n",
      " |      >>> from pprint import pprint\n",
      " |      >>> pprint(mst(wn.synset('bound.a.01'), lambda s:s.also_sees()))\n",
      " |      [Synset('bound.a.01'),\n",
      " |       [Synset('unfree.a.02'),\n",
      " |        [Synset('confined.a.02')],\n",
      " |        [Synset('dependent.a.01')],\n",
      " |        [Synset('restricted.a.01'), [Synset('classified.a.02')]]]]\n",
      " |  \n",
      " |  name(self)\n",
      " |  \n",
      " |  offset(self)\n",
      " |  \n",
      " |  path_similarity(self, other, verbose=False, simulate_root=True)\n",
      " |      Path Distance Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      shortest path that connects the senses in the is-a (hypernym/hypnoym)\n",
      " |      taxonomy. The score is in the range 0 to 1, except in those cases where\n",
      " |      a path cannot be found (will only be true for verbs as there are many\n",
      " |      distinct verb taxonomies), in which case None is returned. A score of\n",
      " |      1 represents identity i.e. comparing a sense with itself will return 1.\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      " |          normally between 0 and 1. None is returned if no connecting path\n",
      " |          could be found. 1 is returned if a ``Synset`` is compared with\n",
      " |          itself.\n",
      " |  \n",
      " |  pos(self)\n",
      " |  \n",
      " |  res_similarity(self, other, ic, verbose=False)\n",
      " |      Resnik Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node).\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects. Synsets whose LCS is the root node of the taxonomy will\n",
      " |          have a score of 0 (e.g. N['dog'][0] and N['table'][0]).\n",
      " |  \n",
      " |  root_hypernyms(self)\n",
      " |      Get the topmost hypernyms of this synset in WordNet.\n",
      " |  \n",
      " |  shortest_path_distance(self, other, simulate_root=False)\n",
      " |      Returns the distance of the shortest path linking the two synsets (if\n",
      " |      one exists). For each synset, all the ancestor nodes and their\n",
      " |      distances are recorded and compared. The ancestor node common to both\n",
      " |      synsets that can be reached with the minimum number of traversals is\n",
      " |      used. If no ancestor nodes are common, None is returned. If a node is\n",
      " |      compared with itself 0 is returned.\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: The Synset to which the shortest path will be found.\n",
      " |      :return: The number of edges in the shortest path connecting the two\n",
      " |          nodes, or None if no path exists.\n",
      " |  \n",
      " |  tree(self, rel, depth=-1, cut_mark=None)\n",
      " |      Return the full relation tree, including self,\n",
      " |      discarding cycles:\n",
      " |      \n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> from pprint import pprint\n",
      " |      >>> computer = wn.synset('computer.n.01')\n",
      " |      >>> topic = lambda s:s.topic_domains()\n",
      " |      >>> pprint(computer.tree(topic))\n",
      " |      [Synset('computer.n.01'), [Synset('computer_science.n.01')]]\n",
      " |      \n",
      " |      UserWarning: Discarded redundant search for Synset('computer.n.01') at depth -3\n",
      " |      \n",
      " |      \n",
      " |      But keep duplicate branches (from 'animal.n.01' to 'entity.n.01'):\n",
      " |      \n",
      " |      >>> dog = wn.synset('dog.n.01')\n",
      " |      >>> hyp = lambda s:s.hypernyms()\n",
      " |      >>> pprint(dog.tree(hyp))\n",
      " |      [Synset('dog.n.01'),\n",
      " |       [Synset('canine.n.02'),\n",
      " |        [Synset('carnivore.n.01'),\n",
      " |         [Synset('placental.n.01'),\n",
      " |          [Synset('mammal.n.01'),\n",
      " |           [Synset('vertebrate.n.01'),\n",
      " |            [Synset('chordate.n.01'),\n",
      " |             [Synset('animal.n.01'),\n",
      " |              [Synset('organism.n.01'),\n",
      " |               [Synset('living_thing.n.01'),\n",
      " |                [Synset('whole.n.02'),\n",
      " |                 [Synset('object.n.01'),\n",
      " |                  [Synset('physical_entity.n.01'),\n",
      " |                   [Synset('entity.n.01')]]]]]]]]]]]]],\n",
      " |       [Synset('domestic_animal.n.01'),\n",
      " |        [Synset('animal.n.01'),\n",
      " |         [Synset('organism.n.01'),\n",
      " |          [Synset('living_thing.n.01'),\n",
      " |           [Synset('whole.n.02'),\n",
      " |            [Synset('object.n.01'),\n",
      " |             [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]\n",
      " |  \n",
      " |  wup_similarity(self, other, verbose=False, simulate_root=True)\n",
      " |      Wu-Palmer Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      depth of the two senses in the taxonomy and that of their Least Common\n",
      " |      Subsumer (most specific ancestor node). Previously, the scores computed\n",
      " |      by this implementation did _not_ always agree with those given by\n",
      " |      Pedersen's Perl implementation of WordNet Similarity. However, with\n",
      " |      the addition of the simulate_root flag (see below), the score for\n",
      " |      verbs now almost always agree but not always for nouns.\n",
      " |      \n",
      " |      The LCS does not necessarily feature in the shortest path connecting\n",
      " |      the two senses, as it is by definition the common ancestor deepest in\n",
      " |      the taxonomy, not closest to the two senses. Typically, however, it\n",
      " |      will so feature. Where multiple candidates for the LCS exist, that\n",
      " |      whose shortest path to the root node is the longest will be selected.\n",
      " |      Where the LCS has multiple paths to the root, the longer path is used\n",
      " |      for the purposes of the calculation.\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects, normally greater than zero. If no connecting path between\n",
      " |          the two senses can be found, None is returned.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _WordNetObject:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a >= b.  Computed by @total_ordering from (not a < b).\n",
      " |  \n",
      " |  __gt__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __le__(self, other, NotImplemented=NotImplemented)\n",
      " |      Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  also_sees(self)\n",
      " |  \n",
      " |  attributes(self)\n",
      " |  \n",
      " |  causes(self)\n",
      " |  \n",
      " |  entailments(self)\n",
      " |  \n",
      " |  hypernyms(self)\n",
      " |  \n",
      " |  hyponyms(self)\n",
      " |  \n",
      " |  in_region_domains(self)\n",
      " |  \n",
      " |  in_topic_domains(self)\n",
      " |  \n",
      " |  in_usage_domains(self)\n",
      " |  \n",
      " |  instance_hypernyms(self)\n",
      " |  \n",
      " |  instance_hyponyms(self)\n",
      " |  \n",
      " |  member_holonyms(self)\n",
      " |  \n",
      " |  member_meronyms(self)\n",
      " |  \n",
      " |  part_holonyms(self)\n",
      " |  \n",
      " |  part_meronyms(self)\n",
      " |  \n",
      " |  region_domains(self)\n",
      " |  \n",
      " |  similar_tos(self)\n",
      " |  \n",
      " |  substance_holonyms(self)\n",
      " |  \n",
      " |  substance_meronyms(self)\n",
      " |  \n",
      " |  topic_domains(self)\n",
      " |  \n",
      " |  usage_domains(self)\n",
      " |  \n",
      " |  verb_groups(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _WordNetObject:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f731bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo = lambda s: s.hyponyms()\n",
    "hyper = lambda s: s.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "806ed322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3661c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('frump.n.01')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('frump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "280b5e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('placental.n.01')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a93ab128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cow.n.02')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "103d7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('restrain.v.01') at depth 2\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('confine.v.03'),\n",
       " Synset('control.v.02'),\n",
       " Synset('hold.v.36'),\n",
       " Synset('inhibit.v.04'),\n",
       " Synset('cabin.v.01'),\n",
       " Synset('closet.v.01'),\n",
       " Synset('coop_up.v.01'),\n",
       " Synset('lock_in.v.01'),\n",
       " Synset('lock_in.v.02'),\n",
       " Synset('bate.v.01'),\n",
       " Synset('catch.v.13'),\n",
       " Synset('countercheck.v.01'),\n",
       " Synset('damp.v.02'),\n",
       " Synset('deny.v.05'),\n",
       " Synset('mortify.v.02'),\n",
       " Synset('restrict.v.02'),\n",
       " Synset('restrict.v.03'),\n",
       " Synset('suppress.v.01'),\n",
       " Synset('thermostat.v.01'),\n",
       " Synset('train.v.09'),\n",
       " Synset('choke.v.07'),\n",
       " Synset('quench.v.03'),\n",
       " Synset('mortify.v.01'),\n",
       " Synset('classify.v.02'),\n",
       " Synset('localize.v.03'),\n",
       " Synset('scant.v.02'),\n",
       " Synset('taboo.v.01'),\n",
       " Synset('baffle.v.03'),\n",
       " Synset('clamp_down.v.01'),\n",
       " Synset('draw_the_line.v.01'),\n",
       " Synset('gate.v.03'),\n",
       " Synset('hamper.v.01'),\n",
       " Synset('inhibit.v.02'),\n",
       " Synset('mark_off.v.01'),\n",
       " Synset('reduce.v.11'),\n",
       " Synset('restrain.v.04'),\n",
       " Synset('rule.v.07'),\n",
       " Synset('stiffen.v.03'),\n",
       " Synset('tie.v.03'),\n",
       " Synset('burke.v.02'),\n",
       " Synset('choke_off.v.01'),\n",
       " Synset('hush.v.02'),\n",
       " Synset('silence.v.02'),\n",
       " Synset('smother.v.03'),\n",
       " Synset('squelch.v.01'),\n",
       " Synset('stifle.v.02'),\n",
       " Synset('wink.v.04'),\n",
       " Synset('trellis.v.01'),\n",
       " Synset('draw.v.36'),\n",
       " Synset('bridle.v.02'),\n",
       " Synset('clog.v.03'),\n",
       " Synset('curb.v.03'),\n",
       " Synset('gag.v.01'),\n",
       " Synset('lull.v.02'),\n",
       " Synset('shout_down.v.01'),\n",
       " Synset('shush.v.01'),\n",
       " Synset('suffocate.v.04'),\n",
       " Synset('suppurate.v.01'),\n",
       " Synset('snaffle.v.02')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('restrain.v.01').closure(lambda s:s.hyponyms(), depth=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3b7a39fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('inhibit.v.04') at depth 2\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('choke.v.07'),\n",
       " Synset('quench.v.03'),\n",
       " Synset('restrain.v.01'),\n",
       " Synset('confine.v.03'),\n",
       " Synset('control.v.02'),\n",
       " Synset('hold.v.36'),\n",
       " Synset('cabin.v.01'),\n",
       " Synset('closet.v.01'),\n",
       " Synset('coop_up.v.01'),\n",
       " Synset('lock_in.v.01'),\n",
       " Synset('lock_in.v.02'),\n",
       " Synset('bate.v.01'),\n",
       " Synset('catch.v.13'),\n",
       " Synset('countercheck.v.01'),\n",
       " Synset('damp.v.02'),\n",
       " Synset('deny.v.05'),\n",
       " Synset('mortify.v.02'),\n",
       " Synset('restrict.v.02'),\n",
       " Synset('restrict.v.03'),\n",
       " Synset('suppress.v.01'),\n",
       " Synset('thermostat.v.01'),\n",
       " Synset('train.v.09'),\n",
       " Synset('mortify.v.01'),\n",
       " Synset('classify.v.02'),\n",
       " Synset('localize.v.03'),\n",
       " Synset('scant.v.02'),\n",
       " Synset('taboo.v.01'),\n",
       " Synset('baffle.v.03'),\n",
       " Synset('clamp_down.v.01'),\n",
       " Synset('draw_the_line.v.01'),\n",
       " Synset('gate.v.03'),\n",
       " Synset('hamper.v.01'),\n",
       " Synset('inhibit.v.02'),\n",
       " Synset('mark_off.v.01'),\n",
       " Synset('reduce.v.11'),\n",
       " Synset('restrain.v.04'),\n",
       " Synset('rule.v.07'),\n",
       " Synset('stiffen.v.03'),\n",
       " Synset('tie.v.03'),\n",
       " Synset('burke.v.02'),\n",
       " Synset('choke_off.v.01'),\n",
       " Synset('hush.v.02'),\n",
       " Synset('silence.v.02'),\n",
       " Synset('smother.v.03'),\n",
       " Synset('squelch.v.01'),\n",
       " Synset('stifle.v.02'),\n",
       " Synset('wink.v.04'),\n",
       " Synset('trellis.v.01'),\n",
       " Synset('draw.v.36'),\n",
       " Synset('bridle.v.02'),\n",
       " Synset('clog.v.03'),\n",
       " Synset('curb.v.03'),\n",
       " Synset('gag.v.01'),\n",
       " Synset('lull.v.02'),\n",
       " Synset('shout_down.v.01'),\n",
       " Synset('shush.v.01'),\n",
       " Synset('suffocate.v.04'),\n",
       " Synset('suppurate.v.01'),\n",
       " Synset('snaffle.v.02')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('inhibit.v.04').closure(lambda s:s.hyponyms(), depth=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c6332c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('suppress.v.04')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('restrain.v.01').hypernyms()[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "62ff4954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('choke.v.07'), Synset('quench.v.03'), Synset('restrain.v.01')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('inhibit.v.04').closure(lambda s:s.hyponyms(), depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f03c87cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('confine.v.03'),\n",
       " Synset('control.v.02'),\n",
       " Synset('hold.v.36'),\n",
       " Synset('inhibit.v.04')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('restrain.v.01').closure(lambda s:s.hyponyms(), depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7003dac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('inhibit.v.04'), Synset('suppress.v.04'), Synset('forget.v.01')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('restrain.v.01').closure(lambda s:s.hypernyms(), depth=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb8c683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('suppress.v.04'), Synset('unlearn.v.01')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('forget.v.01').closure(lambda s:s.hyponyms(), depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7440939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('dog.n.01').closure(lambda s:s.hyponyms(), depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15c43dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wn.synset('spitz.n.01').closure(lambda s:s.hypernyms(), depth=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82541b4c",
   "metadata": {},
   "source": [
    "### Build Abstraction Tree"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6763706",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "Start with a set of concrete words that have embeddings and are in WordNet.\n",
    " - We'll call these Leaves.  All other embeddings are derived from these.\n",
    "\n",
    "[List of Leaf Words]\n",
    "\n",
    "<Attach embedings to these words for later use>\n",
    " - We can put these directly into our abstraction tree\n",
    "\n",
    "[Abstraction Tree]\n",
    "\n",
    "<Find a proper SynSet for each leaf\n",
    "\n",
    "[List of Leaf Synsets]\n",
    "\n",
    "<Trim leaves that are ancestors of other leaves>\n",
    "\n",
    "[List of True Leaf Synsets]\n",
    "\n",
    "<Make sure only the True Leaf Synsets are in our [Abstraction Tree]>\n",
    "\n",
    "<Create a list of unprocessed SynSets>\n",
    "\n",
    "[Unprocessed Synsets]\n",
    "\n",
    "<Start \"processing\" each of these [Unprocessed Synsets]>\n",
    " <Find a hypernym>\n",
    " <Add hypernym to [Unprocessed SynSets]>\n",
    "  <Track some information>\n",
    "   - list of direct children?\n",
    "   - list of children and their depths?\n",
    "   - number of leaves\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "95214337",
   "metadata": {},
   "outputs": [],
   "source": [
    "wac_words = list(wac2vec.keys())\n",
    "wn_words = set(i for i in wn.words())\n",
    "wn_wac_words = wn_words & set(wac_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "957955bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.332, 'reconstruction'),\n",
       " (5.422, 'replay'),\n",
       " (2.258, 'manifold'),\n",
       " (2.318, 'pledge'),\n",
       " (4.397, 'watt')]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(concr_scores.loc[w].RATING, w) for w in list(wn_wac_words)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "bf2b0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RATING = 8 # Abstraction Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c85aeca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concr_scores_subset = concr_scores[concr_scores.RATING >= MIN_RATING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "0ab40e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0271acef73e4a52ad4c974edc23fc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n"
     ]
    }
   ],
   "source": [
    "leaf_words = [w for w in tqdm(wn_wac_words) if w in concr_scores_subset.index]\n",
    "print(len(leaf_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "409e1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Leaf SynSets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "9d8a363f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ed157cbf97415d940d1979f6ef0a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leaf_synsets = [wn.synsets(w)[0] for w in tqdm(leaf_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "f8886f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Abstraction Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "63ea3cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306db205ed6049debec3dcc5ca7243db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = [wac2vec[w] for w in tqdm(leaf_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "052aa98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 393, 393)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_synsets), len(leaf_words), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "e7efda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"SYNSET\" : leaf_synsets,\n",
    "    \"WORD\" : leaf_words,\n",
    "    \"EMBEDDING\" : embeddings\n",
    "}\n",
    "abstraction_tree = pd.DataFrame(data)\n",
    "abstraction_tree.index = abstraction_tree.SYNSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "21146b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstraction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a8876801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get True Leaf SynSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "9dde3d30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fced3741454b3d82f31e94f3b348dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('entity.n.01') at depth 10\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('instrumentality.n.03') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('instrumentality.n.03') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 10\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('artifact.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('matter.n.03') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('entity.n.01') at depth 8\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('structure.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 9\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('artifact.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('substance.n.07') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('matter.n.03') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('entity.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('instrumentality.n.03') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 12\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('substance.n.07') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('matter.n.03') at depth 8\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('entity.n.01') at depth 11\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('instrumentality.n.03') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('artifact.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('entity.n.01') at depth 9\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 11\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 8\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('artifact.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('implement.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('matter.n.03') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('food.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('structure.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('animal.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('musical_instrument.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('body_part.n.01') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('structure.n.01') at depth 5\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/home/crow/anaconda3/envs/nlp2/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:604: UserWarning: Discarded redundant search for Synset('artifact.n.01') at depth 7\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    }
   ],
   "source": [
    "ancestors = set()\n",
    "for s in tqdm(leaf_synsets):\n",
    "    #print(set(s.closure(hyper)))\n",
    "    ancestors = ancestors.union(set(s.closure(hyper)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "c57e8017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "d10f0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_leaf_synsets = list(set(leaf_synsets) - ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "6102136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansestor_leaves = set(leaf_synsets) - set(true_leaf_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "8855a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstraction_tree = abstraction_tree.drop(ansestor_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "ed0a29b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstraction_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58e231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a31eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15d472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "df940e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9b8252a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(str):\n",
    "    def __new__(cls, value, *args, **kwargs):\n",
    "        # explicitly only pass value to the str constructor\n",
    "        return super(Node, cls).__new__(cls, value)\n",
    "\n",
    "    def __init__(self, value, data):\n",
    "        # ... and don't even call the str initializer \n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c0a87561",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Node('test', 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "34477337",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = Tree(a, ['2', '3', Tree('4', ['5', '6'])])\n",
    "t2 = Tree('5', ['2', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bb4e4383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 2]'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1f8258b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,96.0,120.0\" width=\"96px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">test</text></svg><svg width=\"25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">2</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"25%\" x=\"25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">3</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">4</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">5</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">6</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('test', ['2', '3', Tree('4', ['5', '6'])])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c14fdef",
   "metadata": {},
   "source": [
    "t.draw()\n",
    "t.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fcedde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.draw.tree.draw_trees([t1, t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ab698ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '3', Tree('4', ['5', '6'])]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8eff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9bdc0123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tree in module nltk.tree.tree object:\n",
      "\n",
      "class Tree(builtins.list)\n",
      " |  Tree(node, children=None)\n",
      " |  \n",
      " |  A Tree represents a hierarchical grouping of leaves and subtrees.\n",
      " |  For example, each constituent in a syntax tree is represented by a single Tree.\n",
      " |  \n",
      " |  A tree's children are encoded as a list of leaves and subtrees,\n",
      " |  where a leaf is a basic (non-tree) value; and a subtree is a\n",
      " |  nested Tree.\n",
      " |  \n",
      " |      >>> from nltk.tree import Tree\n",
      " |      >>> print(Tree(1, [2, Tree(3, [4]), 5]))\n",
      " |      (1 2 (3 4) 5)\n",
      " |      >>> vp = Tree('VP', [Tree('V', ['saw']),\n",
      " |      ...                  Tree('NP', ['him'])])\n",
      " |      >>> s = Tree('S', [Tree('NP', ['I']), vp])\n",
      " |      >>> print(s)\n",
      " |      (S (NP I) (VP (V saw) (NP him)))\n",
      " |      >>> print(s[1])\n",
      " |      (VP (V saw) (NP him))\n",
      " |      >>> print(s[1,1])\n",
      " |      (NP him)\n",
      " |      >>> t = Tree.fromstring(\"(S (NP I) (VP (V saw) (NP him)))\")\n",
      " |      >>> s == t\n",
      " |      True\n",
      " |      >>> t[1][1].set_label('X')\n",
      " |      >>> t[1][1].label()\n",
      " |      'X'\n",
      " |      >>> print(t)\n",
      " |      (S (NP I) (VP (V saw) (X him)))\n",
      " |      >>> t[0], t[1,1] = t[1,1], t[0]\n",
      " |      >>> print(t)\n",
      " |      (S (X him) (VP (V saw) (NP I)))\n",
      " |  \n",
      " |  The length of a tree is the number of children it has.\n",
      " |  \n",
      " |      >>> len(t)\n",
      " |      2\n",
      " |  \n",
      " |  The set_label() and label() methods allow individual constituents\n",
      " |  to be labeled.  For example, syntax trees use this label to specify\n",
      " |  phrase tags, such as \"NP\" and \"VP\".\n",
      " |  \n",
      " |  Several Tree methods use \"tree positions\" to specify\n",
      " |  children or descendants of a tree.  Tree positions are defined as\n",
      " |  follows:\n",
      " |  \n",
      " |    - The tree position *i* specifies a Tree's *i*\\ th child.\n",
      " |    - The tree position ``()`` specifies the Tree itself.\n",
      " |    - If *p* is the tree position of descendant *d*, then\n",
      " |      *p+i* specifies the *i*\\ th child of *d*.\n",
      " |  \n",
      " |  I.e., every tree position is either a single index *i*,\n",
      " |  specifying ``tree[i]``; or a sequence *i1, i2, ..., iN*,\n",
      " |  specifying ``tree[i1][i2]...[iN]``.\n",
      " |  \n",
      " |  Construct a new tree.  This constructor can be called in one\n",
      " |  of two ways:\n",
      " |  \n",
      " |  - ``Tree(label, children)`` constructs a new tree with the\n",
      " |      specified label and list of children.\n",
      " |  \n",
      " |  - ``Tree.fromstring(s)`` constructs a new tree by parsing the string ``s``.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tree\n",
      " |      builtins.list\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, v)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  __delitem__(self, index)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__ lambda self, other\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__ lambda self, other\n",
      " |  \n",
      " |  __init__(self, node, children=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __le__ lambda self, other\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, v)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__ lambda self, other\n",
      " |      # @total_ordering doesn't work here, since the class inherits from a builtin class\n",
      " |  \n",
      " |  __radd__(self, v)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmul__(self, v)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __setitem__(self, index, value)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  chomsky_normal_form(self, factor='right', horzMarkov=None, vertMarkov=0, childChar='|', parentChar='^')\n",
      " |      This method can modify a tree in three ways:\n",
      " |      \n",
      " |        1. Convert a tree into its Chomsky Normal Form (CNF)\n",
      " |           equivalent -- Every subtree has either two non-terminals\n",
      " |           or one terminal as its children.  This process requires\n",
      " |           the creation of more\"artificial\" non-terminal nodes.\n",
      " |        2. Markov (vertical) smoothing of children in new artificial\n",
      " |           nodes\n",
      " |        3. Horizontal (parent) annotation of nodes\n",
      " |      \n",
      " |      :param factor: Right or left factoring method (default = \"right\")\n",
      " |      :type  factor: str = [left|right]\n",
      " |      :param horzMarkov: Markov order for sibling smoothing in artificial nodes (None (default) = include all siblings)\n",
      " |      :type  horzMarkov: int | None\n",
      " |      :param vertMarkov: Markov order for parent smoothing (0 (default) = no vertical annotation)\n",
      " |      :type  vertMarkov: int | None\n",
      " |      :param childChar: A string used in construction of the artificial nodes, separating the head of the\n",
      " |                        original subtree from the child nodes that have yet to be expanded (default = \"|\")\n",
      " |      :type  childChar: str\n",
      " |      :param parentChar: A string used to separate the node representation from its vertical annotation\n",
      " |      :type  parentChar: str\n",
      " |  \n",
      " |  collapse_unary(self, collapsePOS=False, collapseRoot=False, joinChar='+')\n",
      " |      Collapse subtrees with a single child (ie. unary productions)\n",
      " |      into a new non-terminal (Tree node) joined by 'joinChar'.\n",
      " |      This is useful when working with algorithms that do not allow\n",
      " |      unary productions, and completely removing the unary productions\n",
      " |      would require loss of useful information.  The Tree is modified\n",
      " |      directly (since it is passed by reference) and no value is returned.\n",
      " |      \n",
      " |      :param collapsePOS: 'False' (default) will not collapse the parent of leaf nodes (ie.\n",
      " |                          Part-of-Speech tags) since they are always unary productions\n",
      " |      :type  collapsePOS: bool\n",
      " |      :param collapseRoot: 'False' (default) will not modify the root production\n",
      " |                           if it is unary.  For the Penn WSJ treebank corpus, this corresponds\n",
      " |                           to the TOP -> productions.\n",
      " |      :type collapseRoot: bool\n",
      " |      :param joinChar: A string used to connect collapsed node values (default = \"+\")\n",
      " |      :type  joinChar: str\n",
      " |  \n",
      " |  copy(self, deep=False)\n",
      " |      Return a shallow copy of the list.\n",
      " |  \n",
      " |  draw(self)\n",
      " |      Open a new window containing a graphical diagram of this tree.\n",
      " |  \n",
      " |  flatten(self)\n",
      " |      Return a flat version of the tree, with all non-root non-terminals removed.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> print(t.flatten())\n",
      " |          (S the dog chased the cat)\n",
      " |      \n",
      " |      :return: a tree consisting of this tree's root connected directly to\n",
      " |          its leaves, omitting all intervening non-terminal nodes.\n",
      " |      :rtype: Tree\n",
      " |  \n",
      " |  freeze(self, leaf_freezer=None)\n",
      " |  \n",
      " |  height(self)\n",
      " |      Return the height of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.height()\n",
      " |          5\n",
      " |          >>> print(t[0,0])\n",
      " |          (D the)\n",
      " |          >>> t[0,0].height()\n",
      " |          2\n",
      " |      \n",
      " |      :return: The height of this tree.  The height of a tree\n",
      " |          containing no children is 1; the height of a tree\n",
      " |          containing only leaves is 2; and the height of any other\n",
      " |          tree is one plus the maximum of its children's\n",
      " |          heights.\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  label(self)\n",
      " |      Return the node label of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')\n",
      " |          >>> t.label()\n",
      " |          'S'\n",
      " |      \n",
      " |      :return: the node label (typically a string)\n",
      " |      :rtype: any\n",
      " |  \n",
      " |  leaf_treeposition(self, index)\n",
      " |      :return: The tree position of the ``index``-th leaf in this\n",
      " |          tree.  I.e., if ``tp=self.leaf_treeposition(i)``, then\n",
      " |          ``self[tp]==self.leaves()[i]``.\n",
      " |      \n",
      " |      :raise IndexError: If this tree contains fewer than ``index+1``\n",
      " |          leaves, or if ``index<0``.\n",
      " |  \n",
      " |  leaves(self)\n",
      " |      Return the leaves of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.leaves()\n",
      " |          ['the', 'dog', 'chased', 'the', 'cat']\n",
      " |      \n",
      " |      :return: a list containing this tree's leaves.\n",
      " |          The order reflects the order of the\n",
      " |          leaves in the tree's hierarchical structure.\n",
      " |      :rtype: list\n",
      " |  \n",
      " |  pformat(self, margin=70, indent=0, nodesep='', parens='()', quotes=False)\n",
      " |      :return: A pretty-printed string representation of this tree.\n",
      " |      :rtype: str\n",
      " |      :param margin: The right margin at which to do line-wrapping.\n",
      " |      :type margin: int\n",
      " |      :param indent: The indentation level at which printing\n",
      " |          begins.  This number is used to decide how far to indent\n",
      " |          subsequent lines.\n",
      " |      :type indent: int\n",
      " |      :param nodesep: A string that is used to separate the node\n",
      " |          from the children.  E.g., the default value ``':'`` gives\n",
      " |          trees like ``(S: (NP: I) (VP: (V: saw) (NP: it)))``.\n",
      " |  \n",
      " |  pformat_latex_qtree(self)\n",
      " |      Returns a representation of the tree compatible with the\n",
      " |      LaTeX qtree package. This consists of the string ``\\Tree``\n",
      " |      followed by the tree represented in bracketed notation.\n",
      " |      \n",
      " |      For example, the following result was generated from a parse tree of\n",
      " |      the sentence ``The announcement astounded us``::\n",
      " |      \n",
      " |        \\Tree [.I'' [.N'' [.D The ] [.N' [.N announcement ] ] ]\n",
      " |            [.I' [.V'' [.V' [.V astounded ] [.N'' [.N' [.N us ] ] ] ] ] ] ]\n",
      " |      \n",
      " |      See https://www.ling.upenn.edu/advice/latex.html for the LaTeX\n",
      " |      style file for the qtree package.\n",
      " |      \n",
      " |      :return: A latex qtree representation of this tree.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  pos(self)\n",
      " |      Return a sequence of pos-tagged words extracted from the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.pos()\n",
      " |          [('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]\n",
      " |      \n",
      " |      :return: a list of tuples containing leaves and pre-terminals (part-of-speech tags).\n",
      " |          The order reflects the order of the leaves in the tree's hierarchical structure.\n",
      " |      :rtype: list(tuple)\n",
      " |  \n",
      " |  pprint(self, **kwargs)\n",
      " |      Print a string representation of this Tree to 'stream'\n",
      " |  \n",
      " |  pretty_print(self, sentence=None, highlight=(), stream=None, **kwargs)\n",
      " |      Pretty-print this tree as ASCII or Unicode art.\n",
      " |      For explanation of the arguments, see the documentation for\n",
      " |      `nltk.tree.prettyprinter.TreePrettyPrinter`.\n",
      " |  \n",
      " |  productions(self)\n",
      " |      Generate the productions that correspond to the non-terminal nodes of the tree.\n",
      " |      For each subtree of the form (P: C1 C2 ... Cn) this produces a production of the\n",
      " |      form P -> C1 C2 ... Cn.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.productions() # doctest: +NORMALIZE_WHITESPACE\n",
      " |          [S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',\n",
      " |          NP -> D N, D -> 'the', N -> 'cat']\n",
      " |      \n",
      " |      :rtype: list(Production)\n",
      " |  \n",
      " |  set_label(self, label)\n",
      " |      Set the node label of the tree.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.set_label(\"T\")\n",
      " |          >>> print(t)\n",
      " |          (T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\n",
      " |      \n",
      " |      :param label: the node label (typically a string)\n",
      " |      :type label: any\n",
      " |  \n",
      " |  subtrees(self, filter=None)\n",
      " |      Generate all the subtrees of this tree, optionally restricted\n",
      " |      to trees matching the filter function.\n",
      " |      \n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> for s in t.subtrees(lambda t: t.height() == 2):\n",
      " |          ...     print(s)\n",
      " |          (D the)\n",
      " |          (N dog)\n",
      " |          (V chased)\n",
      " |          (D the)\n",
      " |          (N cat)\n",
      " |      \n",
      " |      :type filter: function\n",
      " |      :param filter: the function to filter all local trees\n",
      " |  \n",
      " |  treeposition_spanning_leaves(self, start, end)\n",
      " |      :return: The tree position of the lowest descendant of this\n",
      " |          tree that dominates ``self.leaves()[start:end]``.\n",
      " |      :raise ValueError: if ``end <= start``\n",
      " |  \n",
      " |  treepositions(self, order='preorder')\n",
      " |          >>> t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
      " |          >>> t.treepositions() # doctest: +ELLIPSIS\n",
      " |          [(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]\n",
      " |          >>> for pos in t.treepositions('leaves'):\n",
      " |          ...     t[pos] = t[pos][::-1].upper()\n",
      " |          >>> print(t)\n",
      " |          (S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))\n",
      " |      \n",
      " |      :param order: One of: ``preorder``, ``postorder``, ``bothorder``,\n",
      " |          ``leaves``.\n",
      " |  \n",
      " |  un_chomsky_normal_form(self, expandUnary=True, childChar='|', parentChar='^', unaryChar='+')\n",
      " |      This method modifies the tree in three ways:\n",
      " |      \n",
      " |        1. Transforms a tree in Chomsky Normal Form back to its\n",
      " |           original structure (branching greater than two)\n",
      " |        2. Removes any parent annotation (if it exists)\n",
      " |        3. (optional) expands unary subtrees (if previously\n",
      " |           collapsed with collapseUnary(...) )\n",
      " |      \n",
      " |      :param expandUnary: Flag to expand unary or not (default = True)\n",
      " |      :type  expandUnary: bool\n",
      " |      :param childChar: A string separating the head node from its children in an artificial node (default = \"|\")\n",
      " |      :type  childChar: str\n",
      " |      :param parentChar: A string separating the node label from its parent annotation (default = \"^\")\n",
      " |      :type  parentChar: str\n",
      " |      :param unaryChar: A string joining two non-terminals in a unary production (default = \"+\")\n",
      " |      :type  unaryChar: str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  convert(tree) from builtins.type\n",
      " |      Convert a tree between different subtypes of Tree.  ``cls`` determines\n",
      " |      which class will be used to encode the new tree.\n",
      " |      \n",
      " |      :type tree: Tree\n",
      " |      :param tree: The tree that should be converted.\n",
      " |      :return: The new Tree.\n",
      " |  \n",
      " |  fromlist(l) from builtins.type\n",
      " |      :type l: list\n",
      " |      :param l: a tree represented as nested lists\n",
      " |      \n",
      " |      :return: A tree corresponding to the list representation ``l``.\n",
      " |      :rtype: Tree\n",
      " |      \n",
      " |      Convert nested lists to a NLTK Tree\n",
      " |  \n",
      " |  fromstring(s, brackets='()', read_node=None, read_leaf=None, node_pattern=None, leaf_pattern=None, remove_empty_top_bracketing=False) from builtins.type\n",
      " |      Read a bracketed tree string and return the resulting tree.\n",
      " |      Trees are represented as nested brackettings, such as::\n",
      " |      \n",
      " |        (S (NP (NNP John)) (VP (V runs)))\n",
      " |      \n",
      " |      :type s: str\n",
      " |      :param s: The string to read\n",
      " |      \n",
      " |      :type brackets: str (length=2)\n",
      " |      :param brackets: The bracket characters used to mark the\n",
      " |          beginning and end of trees and subtrees.\n",
      " |      \n",
      " |      :type read_node: function\n",
      " |      :type read_leaf: function\n",
      " |      :param read_node, read_leaf: If specified, these functions\n",
      " |          are applied to the substrings of ``s`` corresponding to\n",
      " |          nodes and leaves (respectively) to obtain the values for\n",
      " |          those nodes and leaves.  They should have the following\n",
      " |          signature:\n",
      " |      \n",
      " |             read_node(str) -> value\n",
      " |      \n",
      " |          For example, these functions could be used to process nodes\n",
      " |          and leaves whose values should be some type other than\n",
      " |          string (such as ``FeatStruct``).\n",
      " |          Note that by default, node strings and leaf strings are\n",
      " |          delimited by whitespace and brackets; to override this\n",
      " |          default, use the ``node_pattern`` and ``leaf_pattern``\n",
      " |          arguments.\n",
      " |      \n",
      " |      :type node_pattern: str\n",
      " |      :type leaf_pattern: str\n",
      " |      :param node_pattern, leaf_pattern: Regular expression patterns\n",
      " |          used to find node and leaf substrings in ``s``.  By\n",
      " |          default, both nodes patterns are defined to match any\n",
      " |          sequence of non-whitespace non-bracket characters.\n",
      " |      \n",
      " |      :type remove_empty_top_bracketing: bool\n",
      " |      :param remove_empty_top_bracketing: If the resulting tree has\n",
      " |          an empty node label, and is length one, then return its\n",
      " |          single child instead.  This is useful for treebank trees,\n",
      " |          which sometimes contain an extra level of bracketing.\n",
      " |      \n",
      " |      :return: A tree corresponding to the string representation ``s``.\n",
      " |          If this class method is called using a subclass of Tree,\n",
      " |          then it will return a tree of that type.\n",
      " |      :rtype: Tree\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  node\n",
      " |      Outdated method to access the node value; use the label() method instead.\n",
      " |      \n",
      " |      @deprecated: Use label() instead\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.list:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iadd__(self, value, /)\n",
      " |      Implement self+=value.\n",
      " |  \n",
      " |  __imul__(self, value, /)\n",
      " |      Implement self*=value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __reversed__(self, /)\n",
      " |      Return a reverse iterator over the list.\n",
      " |  \n",
      " |  __sizeof__(self, /)\n",
      " |      Return the size of the list in memory, in bytes.\n",
      " |  \n",
      " |  append(self, object, /)\n",
      " |      Append object to the end of the list.\n",
      " |  \n",
      " |  clear(self, /)\n",
      " |      Remove all items from list.\n",
      " |  \n",
      " |  count(self, value, /)\n",
      " |      Return number of occurrences of value.\n",
      " |  \n",
      " |  extend(self, iterable, /)\n",
      " |      Extend list by appending elements from the iterable.\n",
      " |  \n",
      " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      " |      Return first index of value.\n",
      " |      \n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  insert(self, index, object, /)\n",
      " |      Insert object before index.\n",
      " |  \n",
      " |  pop(self, index=-1, /)\n",
      " |      Remove and return item at index (default last).\n",
      " |      \n",
      " |      Raises IndexError if list is empty or index is out of range.\n",
      " |  \n",
      " |  remove(self, value, /)\n",
      " |      Remove first occurrence of value.\n",
      " |      \n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  reverse(self, /)\n",
      " |      Reverse *IN PLACE*.\n",
      " |  \n",
      " |  sort(self, /, *, key=None, reverse=False)\n",
      " |      Sort the list in ascending order and return None.\n",
      " |      \n",
      " |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n",
      " |      order of two equal elements is maintained).\n",
      " |      \n",
      " |      If a key function is given, apply it once to each list item and sort them,\n",
      " |      ascending or descending, according to their function values.\n",
      " |      \n",
      " |      The reverse flag can be set to sort in descending order.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.list:\n",
      " |  \n",
      " |  __class_getitem__(...) from builtins.type\n",
      " |      See PEP 585\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.list:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e6b4dead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1 2 3 (4 5 6))'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.pformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba084e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
